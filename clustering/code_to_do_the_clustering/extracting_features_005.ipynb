{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features, processing them and storing them for the agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/.local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('../../python_scripts/')\n",
    "sys.path.append('../')\n",
    "from networks import largest as Net\n",
    "import h5py\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from utils_clust import normalizing_samples_L2\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import time\n",
    "\n",
    "import pycuda.autoinit\n",
    "import pycuda.gpuarray as gpuarray\n",
    "from skcuda import linalg\n",
    "import skcuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters\n",
    "exp = '083'\n",
    "nb_classes = 16000 # number of classes with which the network was trained\n",
    "\n",
    "mean = [0.383661700858527, 0.3819784115384924, 0.3588786631614881]\n",
    "std=[0.2167717755518767, 0.21201058526724945, 0.21143164036556178]\n",
    "\n",
    "#mean = [0.5, 0.5, 0.5]\n",
    "#std = [0.5, 0.5, 0.5]\n",
    "\n",
    "epochs = ['110']\n",
    "model = '../../saving_model/exp' + exp + '/exp' + exp + '_epoch_' + epochs[0] + '.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from model in: 083\n"
     ]
    }
   ],
   "source": [
    "# some variables\n",
    "feat_side = 2\n",
    "test_number = 'maxpool_allConv'\n",
    "\n",
    "\n",
    "# setting paths\n",
    "# 1st iteration path\n",
    "images_path = '../../surrogate_dataset/training_set_100000/'\n",
    "\n",
    "\n",
    "# 2nd iteration path\n",
    "#images_path = './images_2nd_iteration/'\n",
    "\n",
    "# 3rd iteration path\n",
    "#images_path = './images_3rd_iteration/'\n",
    "\n",
    "# 4th iteration path\n",
    "#images_path = './images_4th_iteration/'\n",
    "\n",
    "\n",
    "output = './less_collisions/'\n",
    "image_names = output + 'features_' + test_number +'.txt'\n",
    "output_path = output + 'features_' + test_number +'.hdf5'\n",
    "\n",
    "print (\"Extracting features from model in: \" + exp)\n",
    "\n",
    "\n",
    "# Load the images\n",
    "image_list = os.listdir(images_path)\n",
    "image_list.sort()\n",
    "nb_images = len(image_list)\n",
    "#nb_images = len(image_list[:64000])\n",
    "#image_list = image_list[:64000]\n",
    "\n",
    "# saving the labels of the images in the labels_path file\n",
    "f_names = open(image_names, 'w')\n",
    "for i in image_list:\n",
    "    f_names.write(i + '\\n')    \n",
    "f_names.close()\n",
    "\n",
    "# instantiating the net\n",
    "net = Net(nb_classes).cuda()\n",
    "\n",
    "# defining transformations:\n",
    "normalize = transforms.Normalize(mean = mean, std=std)\n",
    "transf = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "# loading the model\n",
    "checkpoint = torch.load(model)\n",
    "net.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# We fix the size of the features that we will then flat and concatenate\n",
    "pooling_to_fixed_value = nn.AdaptiveMaxPool2d(feat_side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_layers(out_conv1, out_conv2, out_conv3):\n",
    "    # set the layer config for extracting the features \n",
    "    \n",
    "    out_c1 = pooling_to_fixed_value(out_conv1)\n",
    "    out_c2 = pooling_to_fixed_value(out_conv2)\n",
    "    out_c3 = pooling_to_fixed_value(out_conv3)\n",
    "    out_c = torch.cat([out_c1, out_c2, out_c3], dim = 1)\n",
    "    return out_c\n",
    "\n",
    "def process_features_batch(out_conv1, out_conv2, out_conv3, batch_size):\n",
    "    # process the extracteed features batch by batch (batch_size, nb_feats, height, length)\n",
    "    out_c = process_layers(out_conv1, out_conv2, out_conv3)\n",
    "    out_f = out_c.squeeze().cpu().data.view(batch_size, -1).numpy()\n",
    "    return out_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features and store them. No processing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:46<00:00, 60.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted from ../../saving_model/exp083/exp083_epoch_110.pth.tar!!\n",
      "Features saved in: ./less_collisions/features_maxpool_allConv.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f_h5py = h5py.File(output_path, 'w')\n",
    "\n",
    "batch_size = 10\n",
    "idx = 0\n",
    "images = torch.zeros((batch_size, 3, 96, 96))\n",
    "for iteration in tqdm(range(int(len(image_list)/batch_size))):\n",
    "    \n",
    "    for num, image in enumerate(image_list[idx:idx+batch_size]):\n",
    "        image = Image.open(images_path + image)\n",
    "        image = transf(image)\n",
    "        images[num] = image.float()\n",
    "\n",
    "    samples = Variable(images).cuda()\n",
    "\n",
    "    net.train(False)\n",
    "    (out_conv1, out_conv2, out_conv3) = net.forward_all_conv_feat(samples)\n",
    "\n",
    "    out_f = process_features_batch(out_conv1, out_conv2, out_conv3, batch_size)\n",
    "\n",
    "    if \"/features\" not in f_h5py:\n",
    "        f_h5py.create_dataset('features', shape = (nb_images, out_f.shape[1]), dtype = np.float32)\n",
    "\n",
    "    for feat in out_f:\n",
    "        f_h5py['features'][idx, ...] = feat\n",
    "        idx += 1\n",
    "\n",
    "f_h5py.close()\n",
    "print ('Features extracted from ' + str(model) + '!!')\n",
    "print ('Features saved in: ' + output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

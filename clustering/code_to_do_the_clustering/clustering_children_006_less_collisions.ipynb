{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join surrogate classes that are under the same node\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/.local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from shutil import copyfile, copytree\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../python_scripts')\n",
    "from utils import read_images_stl10 as read_images\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import shutil\n",
    "from os.path import join\n",
    "\n",
    "sys.path.insert(0, './../scikit_learn')\n",
    "sys.path.insert(0, './../')\n",
    "from utils_clust import normalizing_samples_L2, loading_images, searching_similar_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load pairs of children and...\n",
    "iteration_nb = '001_retrieval'\n",
    "childrens_nb = '001'\n",
    "path_pairs_out = './less_collisions/image_pairs' + childrens_nb\n",
    "path_pairs_out_larger_clusters = './less_collisions/image_pairs_mixed_nodes' + childrens_nb\n",
    "\n",
    "# other paths\n",
    "path_target_dset_s_cl = './less_collisions/clusters/dset' + iteration_nb + '_short_cl/'\n",
    "path_target_dset_l_cl = './less_collisions/clusters/dset' + iteration_nb + '_large_cl/'\n",
    "path_new_classes = './less_collisions/clusters/new_classes_' + iteration_nb + '/'\n",
    "path_target_dset_single = './less_collisions/clusters/dset' + iteration_nb + '_clusters_from_single_images'\n",
    "path_single_old_classes = './less_collisions/clusters/old_classes_' + iteration_nb + '/'\n",
    "\n",
    "\n",
    "# source images\n",
    "# 1st iteration path\n",
    "#images_path = '../../surrogate_dataset/classes_folder_16000_set/'\n",
    "images_path = '../../surrogate_dataset/training_set_100000/'\n",
    "# 2nd iteration path\n",
    "#images_path = './images_2nd_iteration/'\n",
    "\n",
    "# 3rd iteration path\n",
    "#images_path = './images_3rd_iteration/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_clusters = []\n",
    "larger_clusters = []\n",
    "\n",
    "if os.path.exists(path_pairs_out  + '.npy'):\n",
    "    short_clusters = np.load(path_pairs_out +'.npy')\n",
    "\n",
    "if os.path.exists(path_pairs_out_larger_clusters  + '.npy'):\n",
    "    larger_clusters = np.load(path_pairs_out_larger_clusters + '.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays loaded stats: \n",
      "children_array: 2317 <type 'numpy.ndarray'> (2317, 2)\n",
      "sub_child_mixed_array: 361 <type 'numpy.ndarray'> (361,)\n"
     ]
    }
   ],
   "source": [
    "print 'Arrays loaded stats: '\n",
    "print \"children_array:\",\n",
    "print len(short_clusters), type(short_clusters), short_clusters.shape\n",
    "\n",
    "#print \"sub_child_int_array:\",\n",
    "#print len(sub_child_int_array)\n",
    "\n",
    "print \"sub_child_mixed_array:\",\n",
    "print len(larger_clusters), type(larger_clusters), larger_clusters.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build first the large clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger cplusters introduced. Length:  979\n",
      "Short clusters (pairs) introduced. Length:  4951\n"
     ]
    }
   ],
   "source": [
    "# We need a set of all the images selected for clustering. \n",
    "# From there we will remove each image when it is added to a cluster.\n",
    "\n",
    "samples_set = set()\n",
    "for cluster_i in larger_clusters:\n",
    "    for sample_i in cluster_i:\n",
    "        samples_set.add(sample_i)\n",
    "        \n",
    "print \"Larger cplusters introduced. Length: \",\n",
    "print len(samples_set)\n",
    "        \n",
    "# I introduce the samples from the simple clusters as well\n",
    "\n",
    "for cluster_i in short_clusters:\n",
    "    for sample_i in cluster_i:\n",
    "        samples_set.add(sample_i)\n",
    "        \n",
    "print \"Short clusters (pairs) introduced. Length: \",\n",
    "print len(samples_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 100000 <type 'set'>\n"
     ]
    }
   ],
   "source": [
    "# we need a set with all the 100k images from the dataset\n",
    "samples_full = os.listdir(images_path)\n",
    "samples_full_set = set([int(sample_i[:-4]) for sample_i in samples_full])\n",
    "\n",
    "print len(samples_full), len(samples_full_set), type(samples_full_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we invert the larger_cluster array. This is to start clustering the larger groups.\n",
    "# This is needed because the tree structure made clusters that are inside other clusters...\n",
    "\n",
    "larger_clusters = larger_clusters[::-1]\n",
    "\n",
    "# Note that we do not need to do that with the simple clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of larger clusters: 294\n"
     ]
    }
   ],
   "source": [
    "cluster_number = 0    # variable to count clusters\n",
    "for cluster_i in larger_clusters:\n",
    "    nb_img = len(cluster_i)\n",
    "    num = 1\n",
    "    temp = 0    # variable to count clusters\n",
    "    for sample_i in cluster_i:       \n",
    "        if sample_i in samples_set:\n",
    "            temp +=1    # variable to count clusters\n",
    "            \n",
    "            ## defining paths \n",
    "            if not os.path.exists(join(path_target_dset_l_cl, 'cl_l_' + str(cluster_number).zfill(6))):\n",
    "                os.makedirs(join(path_target_dset_l_cl, 'cl_l_' + str(cluster_number).zfill(6)))\n",
    "                # 'cl_l' stands for clusters large\n",
    "            \n",
    "            src_path = join(images_path, str(sample_i).zfill(6) + '.png')\n",
    "            dst_path = join(path_target_dset_l_cl, 'cl_l_' + str(cluster_number).zfill(6), str(sample_i).zfill(6) + '.png')\n",
    "            \n",
    "            ## moving files to the new clusterred dataset\n",
    "            shutil.copyfile(src_path, dst_path)\n",
    "            \n",
    "            #### ploting...\n",
    "            #image = Image.open(src_path)\n",
    "            #plt.subplot(1,nb_img, num)\n",
    "            #plt.imshow(np.asarray(image))\n",
    "            #num += 1\n",
    "            \n",
    "            ### removing processed samples\n",
    "            samples_set.remove(sample_i)\n",
    "            samples_full_set.remove(sample_i)\n",
    "    if temp > 0:\n",
    "        cluster_number += 1   # variable to count clusters\n",
    "    plt.show()\n",
    "print \"Number of larger clusters:\",\n",
    "print cluster_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set length after processing the large clusters:  3972\n",
      "Set length of the full set after processing the large clusters:  99021\n"
     ]
    }
   ],
   "source": [
    "print \"Set length after processing the large clusters: \",\n",
    "print len(samples_set)\n",
    "\n",
    "print \"Set length of the full set after processing the large clusters: \",\n",
    "print len(samples_full_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of smaller clusters: 1986\n"
     ]
    }
   ],
   "source": [
    "cluster_number = 0    # variable to count clusters\n",
    "for cluster_i in short_clusters:\n",
    "    nb_img = len(cluster_i)\n",
    "    num = 1\n",
    "    temp = 0    # variable to count clusters\n",
    "    for sample_i in cluster_i:       \n",
    "        if sample_i in samples_set:\n",
    "            temp +=1    # variable to count clusters\n",
    "            \n",
    "            ## defining paths \n",
    "            if not os.path.exists(join(path_target_dset_s_cl, 'cl_s_' + str(cluster_number).zfill(6))):\n",
    "                os.makedirs(join(path_target_dset_s_cl, 'cl_s_' + str(cluster_number).zfill(6)))\n",
    "                # 'cl_l' stands for clusters large\n",
    "            \n",
    "            src_path = join(images_path, str(sample_i).zfill(6) + '.png')\n",
    "            dst_path = join(path_target_dset_s_cl, 'cl_s_' + str(cluster_number).zfill(6), str(sample_i).zfill(6) + '.png')\n",
    "            \n",
    "            ## moving files to the new clusterred dataset\n",
    "            shutil.copyfile(src_path, dst_path)\n",
    "            \n",
    "            ### ploting...\n",
    "            #image = Image.open(images_path + str(sample_i).zfill(6) + '.png')\n",
    "            #plt.subplot(1,nb_img, num)\n",
    "            #plt.imshow(np.asarray(image))\n",
    "            #num += 1.\n",
    "            \n",
    "            ### removing processed samples\n",
    "            samples_set.remove(sample_i)\n",
    "            samples_full_set.remove(sample_i)\n",
    "    if temp > 0:\n",
    "        cluster_number += 1   # variable to count clusters\n",
    "    plt.show()\n",
    "print \"Number of smaller clusters:\",\n",
    "print cluster_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set length after processing the short clusters:  0\n",
      "Set length of the full set after processing the large clusters:  95049\n"
     ]
    }
   ],
   "source": [
    "print \"Set length after processing the short clusters: \",\n",
    "print len(samples_set)\n",
    "\n",
    "print \"Set length of the full set after processing the large clusters: \",\n",
    "print len(samples_full_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduce \"single cluster\" (the images not clustered) to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking numbers...\n",
      "5720\n"
     ]
    }
   ],
   "source": [
    "# first we extract the images that we need to reach the full dataset, in this case 8000\n",
    "max_classes = 8000\n",
    "nb_new_classes = max_classes - (len(os.listdir(path_target_dset_l_cl)) + len(os.listdir(path_target_dset_s_cl)))\n",
    "\n",
    "print \"Checking numbers...\"\n",
    "print nb_new_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5720/5720 [00:11<00:00, 516.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# I will get the imaages from the unsup set\n",
    "single_samples = [image_i for image_i in list(samples_full_set)[:nb_new_classes]]\n",
    "\n",
    "# Save the images in a folder \n",
    "\n",
    "if not os.path.exists(path_new_classes):\n",
    "    os.makedirs(path_new_classes)           \n",
    "\n",
    "for idx in tqdm(single_samples):\n",
    "    path = os.path.join(path_new_classes, str(idx).zfill(6))\n",
    "    image = Image.open(images_path + str(idx).zfill(6) + '.png')\n",
    "    samples_full_set.remove(idx)\n",
    "    image.save(path + '.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set length after processing the short clusters:  0\n",
      "Set length of the full set after processing the large clusters:  89329\n"
     ]
    }
   ],
   "source": [
    "print \"Set length after processing the short clusters: \",\n",
    "print len(samples_set)\n",
    "\n",
    "print \"Set length of the full set after processing the large clusters: \",\n",
    "print len(samples_full_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching the nearest images from unlabeled images - Single images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_images = os.listdir(path_new_classes)\n",
    "single_images.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the features:3440\n",
      "Number of samples:100000\n"
     ]
    }
   ],
   "source": [
    "# We need the features to do the query search\n",
    "\n",
    "# paths\n",
    "features_path = './less_collisions/features_maxpool_allConv.hdf5'\n",
    "\n",
    "# load images\n",
    "samples = loading_images(features_path)\n",
    "# normalize images\n",
    "samples_L2 = normalizing_samples_L2(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_mtx = np.zeros([len(single_images), len(samples_L2[0])])\n",
    "query_names = []\n",
    "for num, sample_i in enumerate(single_images):\n",
    "    query = samples_L2[int(sample_i[:-4])]\n",
    "    query_names.append(sample_i)\n",
    "    query_mtx[num] = query\n",
    "    \n",
    "# compute cosine similarity\n",
    "sim = np.dot(query_mtx, samples_L2.T)\n",
    "\n",
    "# sort ranking\n",
    "ranks = np.argsort(sim, axis=1)[:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_size = 8\n",
    "\n",
    "for num, sample_i in enumerate(query_names):\n",
    "    cluster_i = []\n",
    "    cluster_i.append(sample_i)\n",
    "    \n",
    "    instance_num = 1\n",
    "    while len(cluster_i)<cluster_size:\n",
    "        image_nb = ranks[num][instance_num]\n",
    "        image_name = str(image_nb).zfill(6) + '.png'\n",
    "        if image_nb in samples_full_set:\n",
    "            cluster_i.append(image_name)\n",
    "            samples_full_set.remove(image_nb)\n",
    "        \n",
    "        instance_num += 1\n",
    "    \n",
    "    # move the images fom the cluster_i to the new folder\n",
    "    \n",
    "    src_path = join(path_new_classes, cluster_i[0])\n",
    "    dst_folder =  join(path_target_dset_single, cluster_i[0][:-4])\n",
    "\n",
    "    if not os.path.exists(dst_folder):\n",
    "        os.makedirs(dst_folder)\n",
    "    \n",
    "    dst_path = join(dst_folder, cluster_i[0])\n",
    "    shutil.copyfile(src_path, dst_path)\n",
    "    \n",
    "    for num_cl, img_i in enumerate(cluster_i[1:]):\n",
    "        src_path = join(images_path, cluster_i[num_cl+1])\n",
    "        dst_path = join(dst_folder, img_i)\n",
    "        shutil.copyfile(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set length after processing the short clusters:  0\n",
      "Set length of the full set after processing the large clusters:  49289\n"
     ]
    }
   ],
   "source": [
    "print \"Set length after processing the short clusters: \",\n",
    "print len(samples_set)\n",
    "\n",
    "print \"Set length of the full set after processing the large clusters: \",\n",
    "print len(samples_full_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after that i should do the same with the other clusters with 2 images each..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000,)\n",
      "[1.         0.86025035 0.85134862 0.83130919 0.81856119]\n",
      "[0.82950443 0.83131215 0.82241898 0.85949542 0.82282796]\n",
      "\n",
      "1.0000000000000022\n",
      "0.5091351520829155\n",
      "\n",
      "[62082 69907 28668 90896 69681]\n",
      "[17744 67069 32344 88365     0]\n"
     ]
    }
   ],
   "source": [
    "print sim.shape\n",
    "print sim[:5]\n",
    "print sim[-5:]\n",
    "print ''\n",
    "print max(sim)\n",
    "print min(sim)\n",
    "sim_ord = np.argsort(sim)\n",
    "print ''\n",
    "print sim_ord[:5]\n",
    "print sim_ord[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# finally we move all the single classes to a folder inside the \"dataset00?\" folder\n",
    "list_files = os.listdir(path_new_classes)\n",
    "for sample_i in tqdm(list_files):\n",
    "    src_path = join(path_new_classes, sample_i)\n",
    "    dst_folder =  join(path_target_dset, sample_i[:-4])\n",
    "    \n",
    "    if not os.path.exists(dst_folder):\n",
    "        os.makedirs(dst_folder)\n",
    "    \n",
    "    dst_path = join(dst_folder, sample_i)\n",
    "    shutil.copyfile(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching the nearest images from unlabeled images - Simple clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This one is the same but changing the last cell for some code to search for near images in the 50000 samples left. The aim is get larger clusters for the single images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "features_path = './features_maxpool_allConv.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the features:1792\n",
      "Number of samples:100000\n"
     ]
    }
   ],
   "source": [
    "# load images\n",
    "samples = loading_images(features_path)\n",
    "# normalize images\n",
    "samples_L2 = normalizing_samples_L2(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16315"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_images = len(os.listdir(images_path))\n",
    "current_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "299it [00:00, 687.95it/s]\n"
     ]
    }
   ],
   "source": [
    "unlab_set = read_images('../../data/stl10_binary/unlabeled_X.bin')\n",
    "np.random.seed(42)\n",
    "\n",
    "# indexes drawn in a set to avoid duplicates\n",
    "indexes = set()\n",
    "while len(indexes) < (current_images + nb_new_classes): # we do that to get only the last ones\n",
    "    indexes.add(np.random.randint(unlab_set.shape[0]))\n",
    "    \n",
    "# Save the images in a folder \n",
    "toPill = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "if not os.path.exists(path_new_classes):\n",
    "    os.makedirs(path_new_classes)           \n",
    "\n",
    "for num, idx in tqdm(enumerate(list(indexes)[-nb_new_classes:])):\n",
    "    path = os.path.join(path_new_classes, str(num + current_images).zfill(len(str(max_classes))))\n",
    "    image = toPill(unlab_set[idx])\n",
    "    image.save(path + '.png') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15116/15116 [00:00<00:00, 23543.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# now we copy the images that we did not use during the clustering in another folder\n",
    "if not os.path.exists(path_single_old_classes):\n",
    "    os.makedirs(path_single_old_classes)\n",
    "\n",
    "for sample_i in tqdm(samples_full_set):\n",
    "    src_path = join(images_path, str(sample_i).zfill(5) + '.png')\n",
    "    dst_path = join(path_single_old_classes, str(sample_i).zfill(5) + '.png')\n",
    "    shutil.copyfile(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 299/299 [00:00<00:00, 12496.86it/s]\n",
      "100%|██████████| 15116/15116 [00:01<00:00, 14203.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# finally we move all the single classes to a folder inside the \"dataset00?\" folder\n",
    "list_files = os.listdir(path_new_classes)\n",
    "for sample_i in tqdm(list_files):\n",
    "    src_path = join(path_new_classes, sample_i)\n",
    "    dst_folder =  join(path_target_dset, sample_i[:-4])\n",
    "    \n",
    "    if not os.path.exists(dst_folder):\n",
    "        os.makedirs(dst_folder)\n",
    "    \n",
    "    dst_path = join(dst_folder, sample_i)\n",
    "    shutil.copyfile(src_path, dst_path)\n",
    "\n",
    "\n",
    "list_files = os.listdir(path_single_old_classes)\n",
    "for sample_i in tqdm(list_files):\n",
    "    src_path = join(path_single_old_classes, sample_i)\n",
    "    dst_folder =  join(path_target_dset, sample_i[:-4])\n",
    "    \n",
    "    if not os.path.exists(dst_folder):\n",
    "        os.makedirs(dst_folder)\n",
    "\n",
    "    dst_path = join(dst_folder, sample_i)\n",
    "    shutil.copyfile(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now is time to compute the transformations from the new clustered dataset..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
